#!/usr/bin/env python3
"""
Ultra-fast, universal Oracle→Oracle copier in Python.

- Accepts ANY SQL from a file (no split column required)
- If --split-column is omitted, uses ROWNUM-based chunking (two-level wrapper) to parallelize reads
- If --split-column is provided, uses numeric/date range partitioning
- Parallel pipelines (ThreadPoolExecutor)
- Array/batch fetch & insert tuning (arraysize, prefetchrows, executemany batch)
- Optional APPEND_VALUES (direct path style), NOLOGGING, PARALLEL DML, ALTER TABLE PARALLEL
- Optional source parallel hint: wraps your SQL in SELECT /*+ parallel(N) */ * FROM (<your_sql>)

Usage examples at bottom.
"""

import argparse
import math
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional

import oracledb

# ---------- high-performance defaults (tune to your env) ----------
DEFAULT_WORKERS = min(8, (os.cpu_count() or 8))
DEFAULT_SPLITS = DEFAULT_WORKERS * 4
DEFAULT_BATCH_ROWS = 10000
DEFAULT_ARRAYSIZE = 5000
DEFAULT_PREFETCH = 5000
DEFAULT_COMMIT_EVERY = 200000
# ------------------------------------------------------------------


def init_client_if_available():
    """Enable thick mode if Instant Client is available (few % faster)."""
    try:
        client_dir = os.environ.get("ORACLE_CLIENT_LIB_DIR")
        if client_dir:
            oracledb.init_oracle_client(lib_dir=client_dir)
    except Exception:
        pass  # thin mode is fine


def build_dsn(host, port, service_name):
    return oracledb.makedsn(host, int(port), service_name=service_name)


def read_sql(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read().strip().rstrip(";")


def wrap_with_parallel_hint(sql: str, degree: int) -> str:
    # Hint at the outer select; Oracle will try to parallelize underlying scans where possible.
    return f"SELECT /*+ parallel({int(degree)}) */ * FROM ({sql})"


def get_column_list_from_subquery(pool: oracledb.SessionPool, subquery_sql: str) -> List[str]:
    """Return column names from the *user subquery* (no rn)."""
    probe = f"SELECT * FROM ({subquery_sql}) WHERE 1=0"
    with pool.acquire() as conn, conn.cursor() as cur:
        cur.execute(probe)
        return [d[0] for d in cur.description]


def get_min_max(pool: oracledb.SessionPool, subquery_sql: str, split_column: str):
    sql = f"SELECT MIN({split_column}), MAX({split_column}) FROM ({subquery_sql}) S"
    with pool.acquire() as conn, conn.cursor() as cur:
        cur.arraysize = 1
        cur.prefetchrows = 1
        cur.execute(sql)
        lo, hi = cur.fetchone()
        if lo is None or hi is None:
            raise RuntimeError("Source query returned no rows; cannot compute split ranges.")
        return lo, hi


def get_total_count(pool: oracledb.SessionPool, subquery_sql: str) -> int:
    sql = f"SELECT COUNT(1) FROM ({subquery_sql}) C"
    with pool.acquire() as conn, conn.cursor() as cur:
        cur.arraysize = 1
        cur.prefetchrows = 1
        cur.execute(sql)
        (cnt,) = cur.fetchone()
        return int(cnt or 0)


def make_numeric_ranges(lo, hi, num_ranges: int) -> List[Tuple]:
    """Monotonic ranges [a,b) except last is [a,b]."""
    ranges = []
    step = (hi - lo) / num_ranges
    start = lo
    for i in range(num_ranges):
        end = hi if i == num_ranges - 1 else lo + step * (i + 1)
        ranges.append((start, end))
        start = end
    return ranges


def make_rownum_ranges(total_rows: int, splits: int) -> List[Tuple[int, int]]:
    if total_rows == 0:
        return []
    size = math.ceil(total_rows / splits)
    ranges = []
    lo = 1
    while lo <= total_rows:
        hi = min(total_rows, lo + size - 1)
        ranges.append((lo, hi))
        lo = hi + 1
    return ranges


def worker_insert_loop(
    src_cursor,
    tgt_cursor,
    insert_sql: str,
    batch_rows: int,
    commit_every: int,
    tgt_conn,
) -> int:
    inserted = 0
    since_commit = 0
    while True:
        rows = src_cursor.fetchmany(batch_rows)
        if not rows:
            break
        tgt_cursor.executemany(insert_sql, rows)
        inserted += len(rows)
        since_commit += len(rows)
        if since_commit >= commit_every:
            tgt_conn.commit()
            since_commit = 0
    if since_commit:
        tgt_conn.commit()
    return inserted


def worker_range_splitcol(
    src_pool,
    tgt_pool,
    subquery_sql: str,
    target_table: str,
    cols: List[str],
    split_column: str,
    lo, hi,
    arraysize: int,
    prefetch: int,
    batch_rows: int,
    commit_every: int,
    use_append_values: bool,
    enable_parallel_dml: bool,
) -> int:
    col_list = ", ".join(cols)
    placeholders = ", ".join([f":{i+1}" for i in range(len(cols))])
    hint = "/*+ APPEND_VALUES */ " if use_append_values else ""
    insert_sql = f"INSERT {hint}INTO {target_table} ({col_list}) VALUES ({placeholders})"
    # Use < hi except last worker; we don't know if we are last here, so just use [lo,hi) contract in caller.
    sql = f"SELECT * FROM ({subquery_sql}) WHERE :lo <= {split_column} AND {split_column} < :hi"

    with src_pool.acquire() as sconn, tgt_pool.acquire() as tconn:
        if enable_parallel_dml:
            try:
                with tconn.cursor() as tcur:
                    tcur.execute("ALTER SESSION ENABLE PARALLEL DML")
            except Exception:
                pass
        with sconn.cursor() as scur, tconn.cursor() as tcur:
            scur.arraysize = arraysize
            scur.prefetchrows = prefetch
            tcur.setinputsizes(None)
            scur.execute(sql, dict(lo=lo, hi=hi))
            return worker_insert_loop(scur, tcur, insert_sql, batch_rows, commit_every, tconn)


def worker_range_rownum(
    src_pool,
    tgt_pool,
    subquery_sql: str,
    target_table: str,
    cols: List[str],
    rn_lo: int, rn_hi: int,
    arraysize: int,
    prefetch: int,
    batch_rows: int,
    commit_every: int,
    use_append_values: bool,
    enable_parallel_dml: bool,
) -> int:
    col_list = ", ".join(cols)
    placeholders = ", ".join([f":{i+1}" for i in range(len(cols))])
    hint = "/*+ APPEND_VALUES */ " if use_append_values else ""
    insert_sql = f"INSERT {hint}INTO {target_table} ({col_list}) VALUES ({placeholders})"

    # Two-level ROWNUM pagination (no stable order implied, but non-overlapping and exhaustive):
    # inner: cap to :hi
    inner = f"SELECT t.*, ROWNUM rn FROM ({subquery_sql}) t WHERE ROWNUM <= :hi"
    # outer: keep rows >= :lo
    outer = f"SELECT * FROM ({inner}) WHERE rn >= :lo"

    with src_pool.acquire() as sconn, tgt_pool.acquire() as tconn:
        if enable_parallel_dml:
            try:
                with tconn.cursor() as tcur:
                    tcur.execute("ALTER SESSION ENABLE PARALLEL DML")
            except Exception:
                pass
        with sconn.cursor() as scur, tconn.cursor() as tcur:
            scur.arraysize = arraysize
            scur.prefetchrows = prefetch
            tcur.setinputsizes(None)
            scur.execute(outer, dict(lo=rn_lo, hi=rn_hi))
            return worker_insert_loop(scur, tcur, insert_sql, batch_rows, commit_every, tconn)


def main():
    ap = argparse.ArgumentParser(description="Universal, high-speed Oracle→Oracle copier (any SQL).")
    # Source
    ap.add_argument("--src-user", required=True)
    ap.add_argument("--src-pass", required=True)
    ap.add_argument("--src-host", required=True)
    ap.add_argument("--src-port", default=1521)
    ap.add_argument("--src-service", required=True)
    # Target
    ap.add_argument("--tgt-user", required=True)
    ap.add_argument("--tgt-pass", required=True)
    ap.add_argument("--tgt-host", required=True)
    ap.add_argument("--tgt-port", default=1521)
    ap.add_argument("--tgt-service", required=True)
    # Query & target
    ap.add_argument("--sql-file", required=True, help="Path to SQL file containing the SELECT.")
    ap.add_argument("--target-table", required=True, help="Target table (optionally schema.table).")
    # Parallelization
    ap.add_argument("--split-column", help="Optional numeric/date column for range partitioning.")
    ap.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    ap.add_argument("--splits", type=int, default=DEFAULT_SPLITS)
    # Fetch/insert tuning
    ap.add_argument("--arraysize", type=int, default=DEFAULT_ARRAYSIZE)
    ap.add_argument("--prefetchrows", type=int, default=DEFAULT_PREFETCH)
    ap.add_argument("--batch-rows", type=int, default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit-every", type=int, default=DEFAULT_COMMIT_EVERY)
    # Loader options
    ap.add_argument("--truncate-target", action="store_true")
    ap.add_argument("--nologging", action="store_true")
    ap.add_argument("--append-values", action="store_true", help="Use /*+ APPEND_VALUES */ for direct-path style.")
    ap.add_argument("--enable-parallel-dml", action="store_true", help="ALTER SESSION ENABLE PARALLEL DML on target.")
    ap.add_argument("--target-degree", type=int, default=0, help="ALTER TABLE ... PARALLEL N (0=skip).")
    # Pools
    ap.add_argument("--source-pool-size", type=int, default=0, help="Default=max(workers*2, workers).")
    ap.add_argument("--target-pool-size", type=int, default=0)
    # Source parallel hint
    ap.add_argument("--source-parallel-hint", type=int, default=0, help="Wrap your SQL with /*+ parallel(N) */.")
    args = ap.parse_args()

    init_client_if_available()

    src_dsn = build_dsn(args.src_host, args.src_port, args.src_service)
    tgt_dsn = build_dsn(args.tgt_host, args.tgt_port, args.tgt_service)

    # Pools (cap at workers*2 to avoid session starvation)
    src_pool_max = args.source_pool_size or max(args.workers * 2, args.workers)
    tgt_pool_max = args.target_pool_size or max(args.workers * 2, args.workers)

    src_pool = oracledb.SessionPool(
        user=args.src_user, password=args.src_pass, dsn=src_dsn,
        min=args.workers, max=src_pool_max, increment=max(1, args.workers // 2),
        homogeneous=True, threaded=True, getmode=oracledb.SPOOL_ATTRVAL_WAIT
    )
    tgt_pool = oracledb.SessionPool(
        user=args.tgt_user, password=args.tgt_pass, dsn=tgt_dsn,
        min=args.workers, max=tgt_pool_max, increment=max(1, args.workers // 2),
        homogeneous=True, threaded=True, getmode=oracledb.SPOOL_ATTRVAL_WAIT
    )

    user_sql = read_sql(args.sql_file)
    if args.source_parallel_hint and int(args.source_parallel_hint) > 0:
        subquery_sql = wrap_with_parallel_hint(user_sql, int(args.source_parallel_hint))
    else:
        subquery_sql = user_sql

    # Prep target (optional)
    if args.truncate_target or args.nologging or args.target_degree > 0:
        with tgt_pool.acquire() as conn, conn.cursor() as cur:
            if args.truncate_target:
                print(f"[INIT] TRUNCATE TABLE {args.target_table}")
                cur.execute(f"TRUNCATE TABLE {args.target_table}")
            if args.nologging:
                print(f"[INIT] ALTER TABLE {args.target_table} NOLOGGING")
                try:
                    cur.execute(f"ALTER TABLE {args.target_table} NOLOGGING")
                except Exception as e:
                    print(f"[WARN] NOLOGGING failed: {e}")
            if args.target_degree > 0:
                print(f"[INIT] ALTER TABLE {args.target_table} PARALLEL {int(args.target_degree)}")
                try:
                    cur.execute(f"ALTER TABLE {args.target_table} PARALLEL {int(args.target_degree)}")
                except Exception as e:
                    print(f"[WARN] PARALLEL degree failed: {e}")
            conn.commit()

    # Column list (from the user subquery)
    cols = get_column_list_from_subquery(src_pool, subquery_sql)
    if not cols:
        print("No columns detected from source query; aborting.", file=sys.stderr)
        sys.exit(1)
    print(f"[INFO] Columns: {', '.join(cols)}")

    t0 = time.time()
    total_inserted = 0
    futures = []

    print(f"[CFG ] workers={args.workers} splits={args.splits} arraysize={args.arraysize} "
          f"prefetch={args.prefetchrows} batch={args.batch_rows} commit_every={args.commit_every} "
          f"append_values={args.append_values} rownum_mode={not bool(args.split_column)}")

    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        if args.split_column:
            # numeric/date range partitioning
            lo, hi = get_min_max(src_pool, subquery_sql, args.split_column)
            ranges = make_numeric_ranges(lo, hi, args.splits)
            # Make last range inclusive by nudging hi slightly in the last job
            for i, (rlo, rhi) in enumerate(ranges):
                # last chunk uses <= hi; we emulate by passing rhi for last and < rhi for others
                if i == len(ranges) - 1:
                    # pass a tiny epsilon by not altering; we'll modify SQL to < :hi for all
                    pass
                fut = ex.submit(
                    worker_range_splitcol,
                    src_pool, tgt_pool,
                    subquery_sql, args.target_table, cols, args.split_column,
                    rlo, rhi,
                    args.arraysize, args.prefetchrows,
                    args.batch_rows, args.commit_every,
                    args.append_values, args.enable_parallel_dml,
                )
                futures.append(fut)
        else:
            # ROWNUM chunking
            total = get_total_count(src_pool, subquery_sql)
            print(f"[INFO] Total rows reported by COUNT(*): {total:,}")
            if total == 0:
                print("[DONE] No rows to copy.")
                src_pool.close(); tgt_pool.close()
                return
            ranges = make_rownum_ranges(total, args.splits)
            for rn_lo, rn_hi in ranges:
                fut = ex.submit(
                    worker_range_rownum,
                    src_pool, tgt_pool,
                    subquery_sql, args.target_table, cols,
                    rn_lo, rn_hi,
                    args.arraysize, args.prefetchrows,
                    args.batch_rows, args.commit_every,
                    args.append_values, args.enable_parallel_dml,
                )
                futures.append(fut)

        for fut in as_completed(futures):
            inserted = fut.result()
            total_inserted += inserted
            print(f"[OK  ] chunk done, rows={inserted:,} (total so far={total_inserted:,})")

    # Restore target settings
    if args.nologging or args.target_degree > 0:
        with tgt_pool.acquire() as conn, conn.cursor() as cur:
            if args.nologging:
                try:
                    print(f"[FINAL] ALTER TABLE {args.target_table} LOGGING")
                    cur.execute(f"ALTER TABLE {args.target_table} LOGGING")
                except Exception as e:
                    print(f"[WARN ] restore LOGGING failed: {e}")
            if args.target_degree > 0:
                try:
                    print(f"[FINAL] ALTER TABLE {args.target_table} NOPARALLEL")
                    cur.execute(f"ALTER TABLE {args.target_table} NOPARALLEL")
                except Exception as e:
                    print(f"[WARN ] NOPARALLEL failed: {e}")
            conn.commit()

    elapsed = time.time() - t0
    rps = (total_inserted / elapsed) if elapsed > 0 else 0
    print(f"[DONE] Inserted {total_inserted:,} rows in {elapsed:.2f}s ({rps:,.0f} rows/sec)")

    src_pool.close()
    tgt_pool.close()


if __name__ == "__main__":
    main()

pip install oracledb

SELECT col1, col2, ..., col30
FROM source_schema.source_table
WHERE status = 'ACTIVE'

python ora_copy_universal.py \
  --src-user SRC --src-pass P --src-host src-host --src-port 1521 --src-service SRCSVC \
  --tgt-user TGT --tgt-pass Q --tgt-host tgt-host --tgt-port 1521 --tgt-service TGTSVC \
  --sql-file myquery.sql \
  --target-table target_schema.target_table \
  --workers 8 --splits 32 \
  --arraysize 10000 --prefetchrows 10000 \
  --batch-rows 10000 --commit-every 200000 \
  --truncate-target --nologging --append-values \
  --enable-parallel-dml --target-degree 8 \
  --source-parallel-hint 8

python ora_copy_universal.py \
  ...same connection args... \
  --sql-file myquery.sql \
  --target-table target_schema.target_table \
  --split-column id \
  --workers 8 --splits 32 \
  --arraysize 10000 --prefetchrows 10000 \
  --batch-rows 10000 --commit-every 200000 \
  --truncate-target --nologging --append-values \
  --enable-parallel-dml --target-degree 8 \
  --source-parallel-hint 8



Big picture

The script is a universal Oracle→Oracle copier that can take any SELECT from a .sql file and shovel the results into a target table as fast as Python can push, using:

Parallelism (N worker threads, each running its own Oracle session pair)

Chunking of the source result set (either by a real split column or, if you don’t have one, by ROWNUM windows)

Large batch fetches (arraysize, prefetchrows) to cut round-trips

Array DML (executemany) with big batches and rare commits to minimize logging/overhead

Optional direct-path style inserts (/*+ APPEND_VALUES */), NOLOGGING, PARALLEL DML, and ALTER TABLE PARALLEL

Optional source parallel hint to let Oracle parallelize the SELECT internally

The program is careful to keep reads and writes streaming so memory stays flat while throughput stays high.

Step-by-step logic
1) Parse inputs and set fast defaults

The program reads CLI args (source/target creds & DSNs, workers, splits, arraysize, batch size, etc.).

Defaults are tuned for speed (e.g., workers≈min(8, cpu), arraysize/prefetch≈5k, batch_rows≈10k, commit_every≈200k).

If you set ORACLE_CLIENT_LIB_DIR, it tries thick mode (Instant Client). Otherwise it uses thin mode. (Thick can be a few % faster.)

2) Create two session pools

One pool for source and one for target (oracledb.SessionPool), each sized so threads won’t wait.

Pools are set to threaded=True with WAIT getmode, so workers block until a session is available (no busy loops).

3) Load the user’s SQL and (optionally) wrap it with a parallel hint

Reads --sql-file, strips any trailing semicolon.

If --source-parallel-hint N is given, it wraps your SQL as:

SELECT /*+ parallel(N) */ * FROM (<your_sql>)


That encourages Oracle to use parallel query under the hood. (Doesn’t change results, just speed.)

4) Prepare the target table (optional, but key for speed)

Depending on flags, it can:

TRUNCATE TABLE <target>

ALTER TABLE <target> NOLOGGING

ALTER TABLE <target> PARALLEL <N>

ALTER SESSION ENABLE PARALLEL DML (done inside each worker too as needed)

These reduce redo/logging and allow direct-path and parallel behaviors for very high throughput. (At the end, it restores LOGGING/NOPARALLEL if you asked it to change them.)

5) Discover the column order once

It runs SELECT * FROM (<your_sql>) WHERE 1=0 to grab column metadata (names in order).

That order is used to build the INSERT statement:

INSERT /*+ APPEND_VALUES */ INTO target(col1,...,colN) VALUES(:1,...,:N)


(The APPEND_VALUES hint is optional, controlled by --append-values.)

6) Decide how to chunk the workload
Case A: You provide --split-column

The script queries the source once:

SELECT MIN(split_col), MAX(split_col) FROM (<your_sql>)


It creates numeric/date ranges that cover [min,max] in --splits pieces (aiming for equal sizes).

Each worker gets one range and runs:

SELECT * FROM (<your_sql>)
WHERE :lo <= split_col AND split_col < :hi


This keeps chunks non-overlapping and pushes the pruning to Oracle.

Each worker streams rows and bulk-inserts into the target.

Note: in the provided code the worker uses < :hi for all chunks. That means rows exactly equal to the global MAX(split_col) are not included. Easiest fixes:
• (a) make the last chunk use <= :hi, or
• (b) pass a tiny epsilon to the last chunk’s :hi (e.g., max+1 for integers, max+interval '0.000001' second for timestamps).
I can patch that for you in one line if you want.

Case B: No split column (the universal path)

The script first runs a COUNT:

SELECT COUNT(1) FROM (<your_sql>)


It divides 1..COUNT into --splits ROWNUM windows like [1..100k], [100001..200k], ....

Each worker runs a two-level ROWNUM wrapper (classic, non-overlapping pagination):

-- inner caps the stream at :hi
SELECT t.*, ROWNUM rn FROM (<your_sql>) t WHERE ROWNUM <= :hi
-- outer keeps only rows rn >= :lo
SELECT * FROM ( <inner> ) WHERE rn >= :lo


Because ROWNUM is assigned after predicates are applied in that order, each chunk gets a disjoint slice of the first :hi rows, then peels off the tail >= :lo.

Every worker streams its slice and bulk-inserts.

Important: without an ORDER BY, Oracle is free to choose any stable plan order. For static data and a single plan, chunks are non-overlapping and exhaustive. If rows are changing while you read, or if the plan/order differs across chunks, you can see duplicates/gaps. If you have any deterministic key, add ORDER BY key to your SQL for stable pagination, or prefer a real --split-column.

7) Launch parallel workers (ThreadPoolExecutor)

The main thread builds N futures (one per chunk) and submits them.

Each worker:

Acquires one session from source and one from target pools.

Sets big arraysize and prefetchrows on the source cursor (e.g., 5k–10k+).

Prepares the INSERT with placeholders for array DML.

Executes its SELECT (range predicate or rownum window).

In a loop: fetchmany(batch_rows) → executemany(insert_sql, rows) → occasionally commit().

executemany collapses thousands of rows into a single round-trip.

Commits are intentionally rare (e.g., every 200k rows) to reduce redo and log syncs.

Returns the count inserted.

As workers finish, the main thread aggregates counts and prints progress lines.

8) Cleanup and restore

After all futures complete:

If you asked for NOLOGGING or PARALLEL, it restores:

ALTER TABLE <target> LOGGING
ALTER TABLE <target> NOPARALLEL


Closes both pools.

Prints a throughput line: total rows, seconds, rows/sec.

Why it’s fast

Multiple independent server-side cursors read in parallel → better CPU/IO utilization on the source.

Big fetch arrays shrink network round-trips drastically.

Array DML (executemany) + rare commits cut per-row overhead and redo generation.

Direct-path style (APPEND_VALUES) writes bypass buffer cache (when allowed), laying blocks directly.

NOLOGGING (optional) reduces redo during bulk load.

PARALLEL on the table and PARALLEL DML let Oracle write in multiple slaves.

Source parallel hint nudges Oracle to parallelize the SELECT internally (independent of client threads).

Connection pools avoid reconnect cost and let threads grab sessions quickly.

Important nuances & best practices

ROWNUM chunking vs stability: Without an ORDER BY, the engine’s return order can vary. For truly rock-solid chunking, provide:

a split column (best), or

add a deterministic ORDER BY in your SQL (slower but stable).

Last-chunk inclusion (split-column mode): fix to include the absolute MAX(...) as noted above.

Constraints/triggers: direct-path + NOLOGGING can be blocked by triggers, FKs with immediate checks, etc. If you hit ORA errors, disable --append-values, or load into a staging table then MERGE.

Memory: Everything is streamed. Batches are sized to keep RAM moderate (you control arraysize and batch_rows).

Throughput tuning:

Raise arraysize/prefetchrows (10k–50k) if memory/network allow.

Raise batch_rows (10k–50k) if PGA permits.

Increase workers and splits until either source or target CPUs saturate or you hit I/O/network limits.

Absolute fastest (Oracle-native): When allowed, INSERT /*+ APPEND PARALLEL */ INTO tgt SELECT * FROM src@dblink or Data Pump over DB link will still beat Python—because it keeps the entire transfer inside Oracle’s C engine. Use this script when you need Python or need a universal client-side solution.

What each key function does (map to code)

init_client_if_available() – switches to thick mode if Instant Client is present.

SessionPool(...) – builds source/target pools sized for concurrency.

read_sql() – loads your SELECT.

wrap_with_parallel_hint() – optionally adds /*+ parallel(N) */.

get_column_list_from_subquery() – performs WHERE 1=0 probe to record column order.

get_min_max() – finds min/max of the split column for range partitioning.

get_total_count() – counts rows for ROWNUM windowing.

make_numeric_ranges() – creates [lo, hi) partitions across min..max.

make_rownum_ranges() – creates [lo, hi] ROWNUM windows across 1..COUNT.

worker_range_splitcol() – runs the split-column query and inserts in batches.

worker_range_rownum() – runs the ROWNUM two-level pagination query and inserts in batches.

worker_insert_loop() – the tight loop: fetchmany → executemany → occasional commit.

Want me to patch the last-chunk inclusion?

If you’ll use the --split-column path, I recommend a tiny patch so the last chunk includes the max value (change the last worker’s predicate to <= :hi or bump the last :hi by an epsilon). Say the word and I’ll drop in the exact one-line fix for your data type (NUMBER vs DATE/TIMESTAMP).
