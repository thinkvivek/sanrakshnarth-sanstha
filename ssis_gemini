import xml.etree.ElementTree as ET
import re
import os
import sys
from collections import defaultdict, deque

# --- Configuration ---
# Define the namespaces found in SSIS packages
# Note: The actual namespace URLs might vary slightly in different SSIS versions.
# Inspect your .dtsx file to confirm.
NAMESPACES = {
    'DTS': 'www.microsoft.com/SqlServer/Dts',
    'SQLTask': 'www.microsoft.com/sqlserver/dts/tasks/sqltask',
    # Add other namespaces if needed based on your package components
    'pipeline': 'schemas.microsoft.com/sqlserver/dts/pipeline/wrapper',
}

# Mapping from SSIS component class IDs/names to recognizable types
# These might need adjustment based on your specific SSIS version and components used.
# Use SSIS designer or inspect XML to find correct values for classID or creationName.
COMPONENT_TYPES = {
    "Microsoft.OleDbSource": "OLE DB Source",
    "Microsoft.OledbSource": "OLE DB Source", # Common variations
    "SSIS.Pipeline.Components.OleDbSource": "OLE DB Source",
    "Microsoft.MergeJoin": "Merge Join",
    "SSIS.Pipeline.Components.MergeJoin": "Merge Join",
    "Microsoft.Sort": "Sort",
    "SSIS.Pipeline.Components.Sort": "Sort",
    "Microsoft.DerivedColumn": "Derived Column",
    "SSIS.Pipeline.Components.DerivedColumn": "Derived Column",
    "Microsoft.OleDbDestination": "OLE DB Destination",
    "Microsoft.OledbDestination": "OLE DB Destination", # Common variations
    "SSIS.Pipeline.Components.OleDbDestination": "OLE DB Destination",
    # Add other component types as needed
}

# --- Helper Functions ---

def find_with_ns(element, path):
    """Helper to find elements using registered namespaces."""
    return element.find(path, NAMESPACES)

def findall_with_ns(element, path):
    """Helper to find all elements using registered namespaces."""
    return element.findall(path, NAMESPACES)

def get_component_type(component_elem):
    """Identify the type of a component based on its classID or creationName."""
    class_id = component_elem.get('classID')
    creation_name = component_elem.get('creationName') # Sometimes more readable

    # Prefer creationName if available and mapped
    if creation_name and creation_name in COMPONENT_TYPES:
        return COMPONENT_TYPES[creation_name]

    # Fallback to classID
    if class_id:
        # Sometimes classID includes version info, try matching the start
        for key, value in COMPONENT_TYPES.items():
            if class_id.startswith(key):
                return value

    return "Unknown Component"

def get_property_value(component_elem, prop_name):
    """Find a property value within a component's properties."""
    prop = component_elem.find(f".//DTS:property[@DTS:name='{prop_name}']", NAMESPACES)
    if prop is not None and prop.text:
        return prop.text
    # Sometimes properties are nested differently or use description
    prop_desc = component_elem.find(f".//DTS:property[@description='{prop_name}']", NAMESPACES)
    if prop_desc is not None and prop_desc.text:
        return prop_desc.text
    # Handle specific cases like SQL Command in OLE DB Source
    if prop_name == 'SqlCommand':
         sql_prop = component_elem.find(f".//DTS:property[@description='The SQL command to be executed.']", NAMESPACES)
         if sql_prop is not None and sql_prop.text:
             return sql_prop.text
         # Check AccessMode = 3 (SQL Command from variable) - More complex, needs variable lookup
         # Check AccessMode = 2 (SQL Command) - Common case
         access_mode = get_property_value(component_elem, 'AccessMode')
         if access_mode == '2':
              sql_cmd_prop = get_property_value(component_elem, 'SqlCommand')
              if sql_cmd_prop: return sql_cmd_prop


    return None

def get_input_columns(input_elem):
    """Extracts input columns and their upstream lineage IDs."""
    cols = {}
    ext_props = findall_with_ns(input_elem, ".//DTS:externalMetadataColumn")
    for ext_col in ext_props:
         col_name = ext_col.get('name')
         lineage_id = ext_col.get('lineageId')
         if col_name and lineage_id:
             cols[lineage_id] = col_name
    # Fallback or augment with inputColumns if externalMetadata is incomplete
    in_cols = findall_with_ns(input_elem, ".//DTS:inputColumn")
    for in_col in in_cols:
        lineage_id = in_col.get('lineageId')
        # Sometimes name isn't directly on inputColumn, might need mapping
        # This part might require refinement based on actual package structure
        if lineage_id and lineage_id not in cols:
             # Attempt to find name via external metadata mapping if needed
             pass # Simplified for now
    return cols


def get_output_columns(output_elem):
    """Extracts output columns and their lineage IDs."""
    cols = {}
    out_cols = findall_with_ns(output_elem, ".//DTS:outputColumn")
    for out_col in out_cols:
        name = out_col.get('name')
        lineage_id = out_col.get('lineageId')
        if name and lineage_id:
            cols[lineage_id] = name
    return cols

def map_ssis_join_type(ssis_join_type):
    """Translates SSIS Merge Join numeric type to SQL join keywords."""
    # Based on documentation (may vary slightly):
    # 0: Full Outer Join
    # 1: Left Outer Join
    # 2: Inner Join
    # Right Outer Join isn't a direct type, achieved by swapping inputs + Left Join
    if ssis_join_type == '0':
        return 'FULL OUTER JOIN'
    elif ssis_join_type == '1':
        return 'LEFT OUTER JOIN'
    elif ssis_join_type == '2':
        return 'INNER JOIN'
    else:
        print(f"Warning: Unknown SSIS JoinType '{ssis_join_type}'. Defaulting to INNER JOIN.", file=sys.stderr)
        return 'INNER JOIN' # Default fallback

def sanitize_name(name):
    """Creates a valid SQL temporary table name from a component name."""
    # Remove invalid characters, replace spaces, ensure it starts correctly
    name = re.sub(r'[^\w\s-]', '', name) # Keep alphanumeric, underscore, space, hyphen
    name = re.sub(r'\s+', '_', name)     # Replace spaces with underscores
    name = re.sub(r'-+', '_', name)      # Replace hyphens with underscores
    # Ensure it's not excessively long and starts with #
    return f"#{name[:100]}"

# --- Main Processing Logic ---

def analyze_ssis_package(dtsx_file_path):
    """
    Parses the SSIS package and attempts to generate a SQL script
    for the first Data Flow task found.
    """
    try:
        tree = ET.parse(dtsx_file_path)
        root = tree.getroot()
    except ET.ParseError as e:
        print(f"Error parsing XML file: {e}", file=sys.stderr)
        return None
    except FileNotFoundError:
        print(f"Error: File not found at {dtsx_file_path}", file=sys.stderr)
        return None

    sql_script_lines = []
    sql_script_lines.append("-- Generated SQL Script from SSIS Package Data Flow")
    sql_script_lines.append(f"-- Source File: {os.path.basename(dtsx_file_path)}")
    sql_script_lines.append("-- Generated on: Current time") # Replace with actual date if needed
    sql_script_lines.append("\nSET NOCOUNT ON;\n")

    # Find the first Data Flow Task (Pipeline)
    # DTS:Executable with ExecutableType="SSIS.Pipeline.3"
    data_flow_task = None
    for executable in findall_with_ns(root, ".//DTS:Executable"):
        if executable.get('{www.microsoft.com/SqlServer/Dts}ExecutableType') == "SSIS.Pipeline.3":
            data_flow_task = find_with_ns(executable, ".//DTS:ObjectData/pipeline:main")
            if data_flow_task:
                 print(f"Found Data Flow Task: {executable.get('{www.microsoft.com/SqlServer/Dts}ObjectName', 'Unnamed DFT')}")
                 break # Process the first one found

    if data_flow_task is None:
        print("Error: No Data Flow Task (SSIS.Pipeline.3) found in the package.", file=sys.stderr)
        # Check for Control Flow SQL Tasks as per user note 2
        sql_tasks = findall_with_ns(root, ".//SQLTask:SqlTaskData")
        if sql_tasks:
            print("\n--- Found Control Flow SQL Task Statements ---")
            for i, task_data in enumerate(sql_tasks):
                sql_statement = task_data.get('{www.microsoft.com/sqlserver/dts/tasks/sqltask}SqlStatementSource')
                if sql_statement:
                    sql_script_lines.append(f"-- Control Flow SQL Task {i+1} Statement:")
                    sql_script_lines.append(sql_statement.strip() + ";\n")
                else:
                     sql_script_lines.append(f"-- Control Flow SQL Task {i+1} found, but statement is empty or stored elsewhere (e.g., variable, file).")
            sql_script_lines.append("------------------------------------------\n")
        else:
             return "-- No Data Flow Task or recognizable SQL Task found."


        if not any("-- Control Flow SQL Task" in line for line in sql_script_lines):
             return "-- No Data Flow Task found to convert."
        else:
             # If only control flow tasks were found, return that SQL
             return "\n".join(sql_script_lines)


    components = findall_with_ns(data_flow_task, ".//components/component")
    paths = findall_with_ns(data_flow_task, ".//paths/path")

    component_details = {} # Store parsed info about each component
    component_outputs = {} # Map component refId to its output temp table name
    lineage_map = {}       # Map lineageId to column name and component source refId
    final_destination_details = None
    generated_temp_tables = set()

    # --- Pass 1: Identify components and their basic properties/outputs ---
    print("\n--- Analyzing Components ---")
    for comp in components:
        ref_id = comp.get('refId')
        name = comp.get('name')
        comp_type = get_component_type(comp)
        print(f"Found: {name} ({comp_type}) - RefId: {ref_id}")

        details = {'refId': ref_id, 'name': name, 'type': comp_type, 'inputs': [], 'outputs': []}

        # Extract inputs
        for inp in findall_with_ns(comp, ".//inputs/input"):
             input_ref_id = inp.get('refId')
             input_name = inp.get('name')
             external_metadata_id = inp.get('externalMetadataColumnId') # Link to connection metadata
             # Get input columns linked by lineageId from upstream components
             input_cols = get_input_columns(inp)
             details['inputs'].append({'refId': input_ref_id, 'name': input_name, 'externalId': external_metadata_id, 'columns': input_cols})


        # Extract outputs and map lineage IDs
        for outp in findall_with_ns(comp, ".//outputs/output"):
             output_ref_id = outp.get('refId')
             output_name = outp.get('name')
             output_cols = get_output_columns(outp)
             details['outputs'].append({'refId': output_ref_id, 'name': output_name, 'columns': output_cols})
             # Map lineage IDs from this component's output
             for lin_id, col_name in output_cols.items():
                 lineage_map[lin_id] = {'name': col_name, 'source_refId': ref_id}

        # Extract specific properties based on type
        if comp_type == "OLE DB Source":
            details['sql'] = get_property_value(comp, 'SqlCommand')
            details['table'] = get_property_value(comp, 'OpenRowset') # If using Table/View mode
            access_mode = get_property_value(comp, 'AccessMode')
            details['access_mode'] = access_mode
            if access_mode == '0' or access_mode == '3': # Table/View or SQL from Variable
                 print(f"Warning: OLE DB Source '{name}' uses Table/View or SQL from variable. SQL generation might be incomplete.", file=sys.stderr)


        elif comp_type == "Merge Join":
            details['join_type'] = map_ssis_join_type(get_property_value(comp, 'JoinType'))
            details['join_keys'] = [] # Will populate later using lineage IDs
            # Optimization properties (NumKeys, TreatNullsAsEqual) - could add if needed
            details['left_input_refId'] = None # Will find via paths
            details['right_input_refId'] = None # Will find via paths

        elif comp_type == "Derived Column":
            details['derived_cols'] = []
            for out_col in findall_with_ns(comp, ".//outputs/output/outputColumns/outputColumn"):
                 col_name = out_col.get('name')
                 lineage_id = out_col.get('lineageId')
                 expression_prop = find_with_ns(out_col, ".//properties/property[@name='Expression']")
                 friendly_expr_prop = find_with_ns(out_col, ".//properties/property[@name='FriendlyExpression']")
                 # Prefer FriendlyExpression if available, else raw Expression
                 expression = friendly_expr_prop.text if friendly_expr_prop is not None and friendly_expr_prop.text else expression_prop.text if expression_prop is not None else None

                 if col_name and expression:
                     details['derived_cols'].append({
                         'name': col_name,
                         'lineageId': lineage_id,
                         'expression': expression.strip() if expression else None
                     })

        elif comp_type == "Sort":
             details['sort_keys'] = []
             # Example: <property name="SortKeys" dataType="System.String" state="default" isArray="false">ColumnName1,ColumnName2</property>
             # Note: Sort order (ASC/DESC) might be stored differently, potentially per-column properties. This needs inspection.
             sort_keys_prop = get_property_value(comp, 'SortKeys')
             if sort_keys_prop:
                 details['sort_keys'] = [k.strip() for k in sort_keys_prop.split(',')]
             # SSIS sorts might also remove duplicates - check 'EliminateDuplicates' property if needed
             details['eliminate_duplicates'] = get_property_value(comp, 'EliminateDuplicates') == 'true'


        elif comp_type == "OLE DB Destination":
            details['target_table'] = get_property_value(comp, 'OpenRowset')
            access_mode = get_property_value(comp, 'AccessMode')
            details['access_mode'] = access_mode # e.g., '3' for 'Table or View - fast load'
            details['column_mappings'] = [] # Will populate later

            # Store this as the potential final step
            if final_destination_details is None: # Take the first one found
                final_destination_details = details
            else:
                print(f"Warning: Multiple destination components found. Script currently targets the first one: '{final_destination_details['name']}'.", file=sys.stderr)


        component_details[ref_id] = details

    # --- Pass 2: Map paths and build execution order ---
    print("\n--- Analyzing Paths ---")
    # Store connectivity: upstream_output_refId -> downstream_input_refId
    path_map = {}
    # Store component dependencies: downstream_component_refId -> [upstream_component_refId, ...]
    component_deps = defaultdict(list)
    # Store input mappings for components: component_refId -> {input_refId: upstream_component_refId}
    component_input_sources = defaultdict(dict)
    # Identify source components (no incoming paths)
    all_downstream_inputs = set()
    all_upstream_outputs = set()

    for path in paths:
        start_ref = path.get('startId') # Output refId of upstream component
        end_ref = path.get('endId')     # Input refId of downstream component
        print(f"Path: {start_ref} -> {end_ref}")
        path_map[start_ref] = end_ref
        all_downstream_inputs.add(end_ref)
        all_upstream_outputs.add(start_ref)

        # Find the components connected by this path
        upstream_comp_ref = None
        downstream_comp_ref = None

        # Find upstream component by matching its output refId
        for comp_ref, details in component_details.items():
            for outp in details.get('outputs', []):
                if outp.get('refId') == start_ref:
                    upstream_comp_ref = comp_ref
                    break
            if upstream_comp_ref: break

        # Find downstream component by matching its input refId
        for comp_ref, details in component_details.items():
             for inp in details.get('inputs', []):
                 if inp.get('refId') == end_ref:
                     downstream_comp_ref = comp_ref
                     # Store which upstream component feeds this specific input
                     component_input_sources[downstream_comp_ref][end_ref] = upstream_comp_ref
                     break
             if downstream_comp_ref: break


        if upstream_comp_ref and downstream_comp_ref:
            if downstream_comp_ref not in component_deps[upstream_comp_ref]:
                 component_deps[downstream_comp_ref].append(upstream_comp_ref)
            print(f"  Mapped: Component '{component_details[upstream_comp_ref]['name']}' -> Component '{component_details[downstream_comp_ref]['name']}'")
        else:
            print(f"  Warning: Could not map path {start_ref} -> {end_ref} to components.", file=sys.stderr)


    # Determine execution order using topological sort (simple version for linear flows)
    execution_queue = deque()
    processed_components = set()

    # Find initial source components (those with outputs but whose inputs are not targets of any path)
    source_candidates = set(component_details.keys())
    for downstream_ref in component_input_sources:
         for upstream_ref in component_input_sources[downstream_ref].values():
             if upstream_ref in source_candidates:
                 # If a component feeds another, it might not be an initial source unless it's also a source type
                 pass # Keep potential sources for now


    for comp_ref, details in component_details.items():
         is_source = True
         # A component is a source if it has no inputs defined in the component structure
         # OR if none of its inputs are connected via a path
         if not details.get('inputs'):
             is_source = True
         else:
             has_connected_input = False
             for inp in details['inputs']:
                 if inp['refId'] in all_downstream_inputs:
                     has_connected_input = True
                     break
             if not has_connected_input:
                 is_source = True
             else:
                 is_source = False # It has inputs connected from upstream paths

         # Also explicitly check type if possible
         if details['type'] == 'OLE DB Source':
             is_source = True # Treat OLE DB Sources as starting points

         if is_source and comp_ref not in processed_components:
              execution_queue.append(comp_ref)
              processed_components.add(comp_ref) # Add to processed early for queueing


    print("\n--- Generating SQL ---")
    processed_component_refs_in_order = []

    # Process components in rough dependency order
    # This simple queue works for linear flows, complex graphs need full topological sort
    temp_table_counter = 1
    while execution_queue:
        comp_ref = execution_queue.popleft()
        details = component_details[comp_ref]
        comp_name = details['name']
        comp_type = details['type']
        sql_lines_for_comp = []

        print(f"Processing: {comp_name} ({comp_type})")
        processed_component_refs_in_order.append(comp_ref)


        # --- Generate SQL based on component type ---
        output_temp_table = sanitize_name(f"{comp_name}_Output_{temp_table_counter}")
        temp_table_counter += 1

        if comp_type == "OLE DB Source":
            sql = details.get('sql')
            table = details.get('table')
            access_mode = details.get('access_mode')

            if access_mode == '2' and sql: # SQL Command
                sql_lines_for_comp.append(f"-- Source: {comp_name}")
                sql_lines_for_comp.append(f"PRINT 'INFO: Loading data from Source [{comp_name}]';")
                # Assume the source SQL selects the necessary columns
                # Get output columns to define the temp table structure implicitly
                output_cols_data = details['outputs'][0]['columns'] if details['outputs'] else {}
                col_names = [f"[{c_name}]" for c_name in output_cols_data.values()]

                sql_lines_for_comp.append(f"SELECT {', '.join(col_names) if col_names else '*'}")
                sql_lines_for_comp.append(f"INTO {output_temp_table}")
                sql_lines_for_comp.append(f"FROM ({sql.strip()}) AS Source_{comp_name};")
                sql_lines_for_comp.append("-- TODO: Review source SQL query for correctness and dependencies.")
                sql_lines_for_comp.append("")
                component_outputs[comp_ref] = output_temp_table
                generated_temp_tables.add(output_temp_table)

            elif access_mode == '0' and table: # Table or View
                sql_lines_for_comp.append(f"-- Source: {comp_name} (Table/View: {table})")
                sql_lines_for_comp.append(f"PRINT 'INFO: Loading data from Source [{comp_name}] Table/View [{table}]';")
                output_cols_data = details['outputs'][0]['columns'] if details['outputs'] else {}
                col_names = [f"[{c_name}]" for c_name in output_cols_data.values()]

                sql_lines_for_comp.append(f"SELECT {', '.join(col_names) if col_names else '*'}")
                sql_lines_for_comp.append(f"INTO {output_temp_table}")
                sql_lines_for_comp.append(f"FROM {table};")
                sql_lines_for_comp.append("-- TODO: Ensure source table/view exists and schema matches.")
                sql_lines_for_comp.append("")
                component_outputs[comp_ref] = output_temp_table
                generated_temp_tables.add(output_temp_table)

            else:
                 sql_lines_for_comp.append(f"-- Source: {comp_name}")
                 sql_lines_for_comp.append(f"-- Warning: Unsupported AccessMode ({access_mode}) or missing SQL/Table for OLE DB Source '{comp_name}'. Manual SQL required.")
                 sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for source component [{comp_name}]';")
                 sql_lines_for_comp.append(f"/* -- Placeholder for {comp_name} output:")
                 sql_lines_for_comp.append(f"   CREATE TABLE {output_temp_table} (...); ")
                 sql_lines_for_comp.append(f"   INSERT INTO {output_temp_table} SELECT ... ;")
                 sql_lines_for_comp.append("*/\n")
                 component_outputs[comp_ref] = output_temp_table # Still note the output table


        elif comp_type == "Merge Join":
             sql_lines_for_comp.append(f"-- Merge Join: {comp_name}")
             sql_lines_for_comp.append(f"PRINT 'INFO: Performing Merge Join [{comp_name}]';")

             # Identify left and right inputs based on path mapping stored earlier
             left_input_comp_ref = None
             right_input_comp_ref = None
             input_refs = component_input_sources.get(comp_ref, {})

             # SSIS Merge Join inputs are typically named 'Merge Join Left Input' and 'Merge Join Right Input'
             # We need to find which upstream component connects to which input refId
             left_input_target_refId = None
             right_input_target_refId = None
             for inp in details.get('inputs', []):
                 if 'Left Input' in inp.get('name', ''):
                     left_input_target_refId = inp.get('refId')
                 elif 'Right Input' in inp.get('name', ''):
                     right_input_target_refId = inp.get('refId')

             # Now map the target input refIds back to the upstream component refIds
             for target_refid, upstream_refid in input_refs.items():
                 if target_refid == left_input_target_refId:
                     left_input_comp_ref = upstream_refid
                 elif target_refid == right_input_target_refId:
                     right_input_comp_ref = upstream_refid

             if not left_input_comp_ref or not right_input_comp_ref:
                 sql_lines_for_comp.append(f"-- ERROR: Could not determine Left or Right input tables for Merge Join '{comp_name}'. Manual SQL required.")
                 sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for merge join component [{comp_name}]';\n")
                 component_outputs[comp_ref] = None # Indicate failure
             else:
                 left_table = component_outputs.get(left_input_comp_ref)
                 right_table = component_outputs.get(right_input_comp_ref)
                 join_type = details['join_type']

                 if not left_table or not right_table:
                     sql_lines_for_comp.append(f"-- ERROR: Input tables ('{left_table}', '{right_table}') for Merge Join '{comp_name}' not found (upstream component failed?). Manual SQL required.")
                     sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for merge join component [{comp_name}] due to missing inputs';\n")
                     component_outputs[comp_ref] = None
                 else:
                     # Extract Join Keys - Requires mapping lineage IDs back to column names IN THE INPUT TABLES
                     join_conditions = []
                     # Properties like 'JoinKeyLeft','JoinKeyRight' might exist, or check inputColumn properties
                     # This part is complex: need to look at inputColumns on both Left and Right inputs
                     # and find the lineageIds specified for the join, then map those lineageIds back
                     # to column names in the *upstream* components' outputs.

                     # Simplified approach: Assume JoinKeys property exists (might need adjustment)
                     # <property name="JoinKeysLeft">L_LineageID1;L_LineageID2</property>
                     # <property name="JoinKeysRight">R_LineageID1;R_LineageID2</property>
                     left_keys_str = get_property_value(comp, 'JoinKeysLeft')
                     right_keys_str = get_property_value(comp, 'JoinKeysRight')

                     if left_keys_str and right_keys_str:
                         left_key_lineage_ids = left_keys_str.split(';')
                         right_key_lineage_ids = right_keys_str.split(';')

                         if len(left_key_lineage_ids) == len(right_key_lineage_ids):
                             for i in range(len(left_key_lineage_ids)):
                                 left_lin_id = left_key_lineage_ids[i]
                                 right_lin_id = right_key_lineage_ids[i]

                                 # Find column names corresponding to these lineage IDs in the INPUT components
                                 left_col_name = lineage_map.get(left_lin_id, {}).get('name')
                                 right_col_name = lineage_map.get(right_lin_id, {}).get('name')

                                 if left_col_name and right_col_name:
                                     join_conditions.append(f"L.[{left_col_name}] = R.[{right_col_name}]")
                                 else:
                                     join_conditions = [] # Invalidate if any key mapping fails
                                     sql_lines_for_comp.append(f"-- WARNING: Could not map join key lineage IDs ({left_lin_id}, {right_lin_id}) to column names for Merge Join '{comp_name}'.")
                                     print(f"Warning: Failed mapping join key lineage IDs {left_lin_id}/{right_lin_id} for {comp_name}", file=sys.stderr)
                                     break
                         else:
                             sql_lines_for_comp.append(f"-- WARNING: Mismatched number of left/right join keys for Merge Join '{comp_name}'.")

                     if not join_conditions:
                          sql_lines_for_comp.append(f"-- ERROR: Failed to determine join keys for Merge Join '{comp_name}'. Manual SQL required.")
                          sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for merge join component [{comp_name}] join keys';\n")
                          component_outputs[comp_ref] = None
                     else:
                        # Determine output columns - SELECT list
                        select_cols = []
                        output_cols_data = details['outputs'][0]['columns'] if details['outputs'] else {}
                        if not output_cols_data:
                             sql_lines_for_comp.append(f"-- WARNING: Could not determine output columns for Merge Join '{comp_name}'. Selecting all from both inputs.")
                             select_cols.append("L.*")
                             select_cols.append("R.*")
                        else:
                            # Map output lineage IDs back to inputs to decide L.* or R.* or specific columns
                            # This requires tracing lineage through the join component's inputs/outputs
                            # Simplification: Select needed columns based on output definition, prefixing with L. or R.
                            # Need to know which input each output column originally came from.
                            print(f"Info: Defining SELECT list for Merge Join {comp_name}. This requires mapping output columns back to inputs.")

                            left_input_cols = {}
                            right_input_cols = {}
                            if details['inputs']:
                                if len(details['inputs']) > 0 : left_input_cols = details['inputs'][0].get('columns', {})
                                if len(details['inputs']) > 1 : right_input_cols = details['inputs'][1].get('columns', {})


                            for lin_id, col_name in output_cols_data.items():
                                # Check which input this lineageId belongs to
                                if lin_id in left_input_cols:
                                    upstream_col_name = left_input_cols[lin_id]
                                    select_cols.append(f"L.[{upstream_col_name}] AS [{col_name}]")
                                elif lin_id in right_input_cols:
                                    upstream_col_name = right_input_cols[lin_id]
                                    # Avoid selecting duplicate join key columns if possible (e.g., if join key name is same on both sides)
                                    is_right_join_key = False
                                    if right_keys_str:
                                        for r_key_lin_id in right_keys_str.split(';'):
                                            if lin_id == r_key_lin_id:
                                                # Check if the corresponding left key has the same *output* name
                                                left_key_idx = right_keys_str.split(';').index(r_key_lin_id)
                                                left_key_lin_id = left_keys_str.split(';')[left_key_idx]
                                                left_key_name_out = output_cols_data.get(left_key_lin_id)
                                                if left_key_name_out == col_name:
                                                    is_right_join_key = True
                                                    break
                                    if not is_right_join_key: # Only add if not a duplicate join key already added from left
                                         select_cols.append(f"R.[{upstream_col_name}] AS [{col_name}]")
                                    # Else: skip R.[key] if L.[key] AS [key] was already added

                                else:
                                    print(f"Warning: Output column '{col_name}' (LineageID: {lin_id}) in Merge Join '{comp_name}' could not be traced back to Left/Right input.", file=sys.stderr)
                                    select_cols.append(f"NULL AS [{col_name}] -- TODO: Verify source for this column")


                        if not select_cols:
                           sql_lines_for_comp.append(f"-- ERROR: Could not determine any output columns for Merge Join '{comp_name}'. Manual SQL required.")
                           sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for merge join component [{comp_name}] output columns';\n")
                           component_outputs[comp_ref] = None
                        else:
                            sql_lines_for_comp.append(f"SELECT {', '.join(select_cols)}")
                            sql_lines_for_comp.append(f"INTO {output_temp_table}")
                            sql_lines_for_comp.append(f"FROM {left_table} AS L")
                            sql_lines_for_comp.append(f"{join_type} {right_table} AS R")
                            sql_lines_for_comp.append(f"  ON {' AND '.join(join_conditions)}")
                            # SSIS Merge Join requires sorted input. SQL doesn't strictly, but we note it.
                            sql_lines_for_comp.append(f"-- INFO: SSIS Merge Join requires sorted inputs. Corresponding Sort components should precede this step.")
                            sql_lines_for_comp.append("")
                            component_outputs[comp_ref] = output_temp_table
                            generated_temp_tables.add(output_temp_table)


        elif comp_type == "Sort":
            sql_lines_for_comp.append(f"-- Sort: {comp_name}")
            sql_lines_for_comp.append(f"PRINT 'INFO: Applying Sort [{comp_name}]';")
            # Find the input component
            input_comp_ref = None
            input_target_refId = details['inputs'][0]['refId'] if details['inputs'] else None
            if input_target_refId and component_input_sources.get(comp_ref):
                 input_comp_ref = component_input_sources[comp_ref].get(input_target_refId)

            input_table = component_outputs.get(input_comp_ref) if input_comp_ref else None

            if not input_table:
                sql_lines_for_comp.append(f"-- ERROR: Input table for Sort '{comp_name}' not found. Manual SQL required.")
                sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for sort component [{comp_name}]';\n")
                component_outputs[comp_ref] = None
            else:
                sort_keys = details.get('sort_keys', [])
                eliminate_duplicates = details.get('eliminate_duplicates', False)

                # Get all columns from the input table for the SELECT list
                # This requires knowing the schema of the input temp table.
                # Simplification: Assume we select '*' and handle ordering/distinct later.
                # A better approach would be to track columns through lineage_map.
                output_cols_data = details['outputs'][0]['columns'] if details['outputs'] else {}
                select_cols = [f"[{col_name}]" for col_name in output_cols_data.values()] if output_cols_data else ["*"]


                sql_lines_for_comp.append(f"SELECT {'DISTINCT ' if eliminate_duplicates else ''}{', '.join(select_cols)}")
                sql_lines_for_comp.append(f"INTO {output_temp_table}")
                sql_lines_for_comp.append(f"FROM {input_table}")

                if sort_keys:
                    # Need to map sort key names (which might be output names of Sort)
                    # back to the names in the input_table if they differ.
                    # Using output column names directly for ORDER BY assuming they pass through.
                    # TODO: Add ASC/DESC modifiers if available in SSIS properties. Requires parsing sort flags.
                    order_by_clauses = [f"[{key}] ASC" for key in sort_keys] # Assume ASC default
                    sql_lines_for_comp.append(f"ORDER BY {', '.join(order_by_clauses)}")
                elif eliminate_duplicates:
                     # If DISTINCT is used without ORDER BY, results are non-deterministic
                     # Usually SSIS Sort implies ordering, so this case might indicate incomplete parsing
                     sql_lines_for_comp.append("-- WARNING: Sort component eliminated duplicates but has no explicit sort keys. Order is not guaranteed.")

                sql_lines_for_comp.append(";\n")
                component_outputs[comp_ref] = output_temp_table
                generated_temp_tables.add(output_temp_table)


        elif comp_type == "Derived Column":
            sql_lines_for_comp.append(f"-- Derived Column: {comp_name}")
            sql_lines_for_comp.append(f"PRINT 'INFO: Applying Derived Columns [{comp_name}]';")
            # Find the input component
            input_comp_ref = None
            input_target_refId = details['inputs'][0]['refId'] if details['inputs'] else None
            if input_target_refId and component_input_sources.get(comp_ref):
                 input_comp_ref = component_input_sources[comp_ref].get(input_target_refId)

            input_table = component_outputs.get(input_comp_ref) if input_comp_ref else None

            if not input_table:
                sql_lines_for_comp.append(f"-- ERROR: Input table for Derived Column '{comp_name}' not found. Manual SQL required.")
                sql_lines_for_comp.append(f"PRINT 'ERROR: Manual SQL required for derived column component [{comp_name}]';\n")
                component_outputs[comp_ref] = None
            else:
                # Get columns from the input source component
                input_comp_details = component_details.get(input_comp_ref)
                input_cols = {}
                if input_comp_details and input_comp_details['outputs']:
                    input_cols = input_comp_details['outputs'][0].get('columns', {})

                select_list = []
                existing_output_cols = set()

                # Add existing columns from input (pass-through)
                # Need to check which input columns are actually used downstream or are part of the final output
                # Simplification: Select all non-replaced input columns + new derived columns.
                # Get all output columns defined for THIS component
                current_output_cols = details['outputs'][0]['columns'] if details['outputs'] else {}

                # Map input lineage IDs to their names
                input_lineage_to_name = {lin_id: name for lin_id, name in input_cols.items()}


                # Identify which columns from the input are passed through
                for out_lin_id, out_col_name in current_output_cols.items():
                     # Is this output column derived in *this* step?
                     is_derived_here = False
                     for der_col in details.get('derived_cols', []):
                         if der_col['lineageId'] == out_lin_id:
                             is_derived_here = True
                             break
                     if not is_derived_here:
                         # If not derived here, it must come from the input. Find its original name.
                         # This requires tracing lineage back. The inputColumn elements might help here.
                         input_lineage_ref = None
                         for inp_col in findall_with_ns(comp, f".//inputs/input/inputColumns/inputColumn[@lineageId='{out_lin_id}']"):
                             # This might be too simplistic, mapping could be complex
                             input_lineage_ref = out_lin_id # Assume pass-through lineage ID is same
                             break

                         if input_lineage_ref and input_lineage_ref in input_lineage_to_name:
                             input_col_name = input_lineage_to_name[input_lineage_ref]
                             # Handle renaming: If output name is different from input name
                             if input_col_name != out_col_name:
                                select_list.append(f"[{input_col_name}] AS [{out_col_name}]")
                             else:
                                select_list.append(f"[{input_col_name}]")
                             existing_output_cols.add(out_col_name)

                         else:
                             print(f"Warning: Could not trace input source for pass-through column '{out_col_name}' (Lineage: {out_lin_id}) in Derived Column '{comp_name}'.", file=sys.stderr)
                             select_list.append(f"NULL AS [{out_col_name}] -- TODO: Verify pass-through source")
                             existing_output_cols.add(out_col_name)


                # Add new derived columns
                derived_col_expressions = []
                for der_col in details.get('derived_cols', []):
                    col_name = der_col['name']
                    expression = der_col['expression']
                    lineage_id = der_col['lineageId']

                    # --- SSIS Expression Translation (VERY Basic) ---
                    # This is the most complex part and likely needs significant manual work or a dedicated library.
                    # Replace SSIS variables (e.g., @[User::MyVar]) - Needs variable value lookup (not implemented)
                    expression = re.sub(r'@\[User::(\w+)\]', r'/* TODO: Replace SSIS Variable @[User::\1] */ NULL', expression)
                    expression = re.sub(r'@\[System::(\w+)\]', r'/* TODO: Replace SSIS Variable @[System::\1] */ NULL', expression)

                    # Replace SSIS functions with rough T-SQL equivalents (highly approximate)
                    expression = expression.replace("DT_WSTR", "CAST") # Needs length specifier
                    expression = expression.replace("DT_DBTIMESTAMP", "CAST") # Needs AS DATETIME etc.
                    expression = expression.replace("==", "=") # Equality check
                    expression = expression.replace("!=", "<>") # Inequality
                    expression = expression.replace("&&", "AND") # Logical AND
                    expression = expression.replace("||", "OR")  # Logical OR
                    expression = expression.replace("(DT_WSTR,50)", "") # Remove simple casts for now
                    expression = expression.replace("ISNULL", "ISNULL") # SQL ISNULL often takes 2 args
                    expression = expression.replace("LEN", "LEN") # Often compatible
                    expression = expression.replace("SUBSTRING", "SUBSTRING") # Often compatible (check args)
                    expression = expression.replace("FINDSTRING", "CHARINDEX") # Arguments might differ
                    expression = expression.replace("REPLACE", "REPLACE") # Often compatible
                    expression = expression.replace("UPPER", "UPPER")
                    expression = expression.replace("LOWER", "LOWER")
                    expression = expression.replace("TRIM", "TRIM") # SQL TRIM is newer, LTRIM(RTRIM()) is safer
                    expression = expression.replace("GETDATE", "GETDATE") # Compatible
                    expression = expression.replace("DATEADD", "DATEADD") # Check args ('ss', 'mi', 'hh', etc.)
                    expression = expression.replace("DATEDIFF", "DATEDIFF") # Check args

                    # Replace column references [Col Name] or just ColName based on lineage map
                    # Find lineage IDs used in the expression and replace with actual column names from input_cols
                    def replace_col_ref(match):
                        potential_col_name = match.group(1)
                        # Find the input column name that corresponds to this name in the expression
                        # This assumes expression uses the *input* column names.
                        # SSIS sometimes uses lineage IDs directly in expressions - needs handling.
                        # Basic match for now:
                        found = False
                        for lin_id, inp_name in input_cols.items():
                             if inp_name == potential_col_name:
                                 found = True
                                 break
                        if found:
                             return f"[{potential_col_name}]" # Quote it for SQL
                        else:
                             # Maybe it's a function or literal, leave it.
                             # Or maybe it's a column reference using a different naming convention.
                             # This is where detailed lineage tracing is vital.
                             print(f"Warning: Unrecognized identifier '{potential_col_name}' in expression for '{col_name}' in '{comp_name}'. Assuming literal or function.", file=sys.stderr)
                             return potential_col_name # Return as is

                    # Attempt to replace bare words or bracketed words that match input columns
                    expression = re.sub(r'\[([^\]]+)\]', lambda m: f"[{input_lineage_to_name.get(m.group(1), m.group(1))}]" if m.group(1) in input_lineage_to_name else m.group(0), expression) # Handle bracketed names first
                    # expression = re.sub(r'\b([a-zA-Z_][\w]*)\b', replace_col_ref, expression) # Handle bare names (can be risky)


                    sql_expr = f"{expression} AS [{col_name}]"
                    derived_col_expressions.append(sql_expr)
                    existing_output_cols.add(col_name) # Mark as handled

                # Combine existing and derived columns
                final_select_list = select_list + derived_col_expressions

                if not final_select_list:
                     sql_lines_for_comp.append(f"-- WARNING: No columns identified for output of Derived Column '{comp_name}'. Selecting *.")
                     final_select_list.append("*")


                sql_lines_for_comp.append(f"SELECT {', '.join(final_select_list)}")
                sql_lines_for_comp.append(f"INTO {output_temp_table}")
                sql_lines_for_comp.append(f"FROM {input_table};")
                sql_lines_for_comp.append("-- TODO: Review translated SSIS expressions for correctness.")
                sql_lines_for_comp.append("")
                component_outputs[comp_ref] = output_temp_table
                generated_temp_tables.add(output_temp_table)


        elif comp_type == "OLE DB Destination":
             # This is handled last, after processing all upstream components
             # Store details for final INSERT statement generation
             final_destination_details = details # Update in case this is later in the flow than one found earlier
             # We don't create a temp table *for* the destination, we use the input table
             component_outputs[comp_ref] = component_outputs.get(component_deps.get(comp_ref, [None])[0]) # Output is conceptually the input table


        else:
            sql_lines_for_comp.append(f"-- Component: {comp_name} ({comp_type})")
            sql_lines_for_comp.append(f"-- Warning: SQL generation for component type '{comp_type}' is not implemented.")
            sql_lines_for_comp.append(f"PRINT 'WARNING: SQL generation not implemented for component [{comp_name}] type [{comp_type}]';")
            # Try to pass data through if possible
            input_comp_ref = component_deps.get(comp_ref, [None])[0]
            input_table = component_outputs.get(input_comp_ref)
            if input_table:
                sql_lines_for_comp.append(f"-- Attempting to pass data through from {input_table}")
                sql_lines_for_comp.append(f"SELECT * INTO {output_temp_table} FROM {input_table}; -- Placeholder pass-through")
                sql_lines_for_comp.append("")
                component_outputs[comp_ref] = output_temp_table
                generated_temp_tables.add(output_temp_table)

            else:
                 sql_lines_for_comp.append(f"PRINT 'ERROR: Cannot pass data through for [{comp_name}], input table missing.';\n")
                 component_outputs[comp_ref] = None


        sql_script_lines.extend(sql_lines_for_comp)

        # Add downstream components that depend on this one to the queue if not already processed/queued
        # Find components whose input is connected to this component's output
        for downstream_ref, upstreams in component_input_sources.items():
            if comp_ref in upstreams.values(): # If this component is an input source for downstream_ref
                 # Check if *all* inputs for the downstream component are now ready
                 all_inputs_ready = True
                 downstream_comp_details = component_details[downstream_ref]
                 required_upstream_refs = set(component_input_sources.get(downstream_ref, {}).values())

                 for upstream_ref in required_upstream_refs:
                     if upstream_ref not in component_outputs or component_outputs[upstream_ref] is None:
                         all_inputs_ready = False
                         #print(f"Debug: Downstream '{downstream_comp_details['name']}' waiting for upstream '{component_details[upstream_ref]['name']}'")
                         break

                 if all_inputs_ready and downstream_ref not in processed_components:
                     # Check if already in queue to prevent duplicates
                     already_queued = False
                     for item in execution_queue:
                         if item == downstream_ref:
                             already_queued = True
                             break
                     if not already_queued:
                         execution_queue.append(downstream_ref)
                     processed_components.add(downstream_ref) # Mark as queued/processed


    # --- Final Step: Generate INSERT for Destination ---
    if final_destination_details:
        dest_name = final_destination_details['name']
        target_table = final_destination_details.get('target_table')
        access_mode = final_destination_details.get('access_mode') # e.g., 3 = Table/View - fast load

        sql_script_lines.append(f"-- Destination: {dest_name}")

        # Find the input component feeding the destination
        dest_input_comp_ref = None
        dest_input_target_refId = final_destination_details['inputs'][0]['refId'] if final_destination_details.get('inputs') else None

        if dest_input_target_refId and component_input_sources.get(final_destination_details['refId']):
            dest_input_comp_ref = component_input_sources[final_destination_details['refId']].get(dest_input_target_refId)

        source_temp_table = component_outputs.get(dest_input_comp_ref) if dest_input_comp_ref else None


        if not target_table:
            sql_script_lines.append(f"-- ERROR: Target table not identified for destination '{dest_name}'. Manual INSERT required.")
            sql_script_lines.append(f"PRINT 'ERROR: Manual INSERT required for destination component [{dest_name}] - Target table missing.';\n")
        elif not source_temp_table:
            sql_script_lines.append(f"-- ERROR: Input temp table for destination '{dest_name}' not found (upstream component failed?). Manual INSERT required.")
            sql_script_lines.append(f"PRINT 'ERROR: Manual INSERT required for destination component [{dest_name}] - Input data missing.';\n")
        elif access_mode not in ['0', '3']: # 0=Table/View, 3=Table/View FastLoad
             sql_script_lines.append(f"-- WARNING: Destination '{dest_name}' uses AccessMode {access_mode}, which might not be a simple table insert. Assuming standard INSERT.")
             sql_script_lines.append(f"PRINT 'WARNING: Assuming standard INSERT for destination [{dest_name}] with AccessMode {access_mode}';")
             # Fall through to generate standard INSERT anyway as a best guess

        # Proceed if we have target table and source temp table
        if target_table and source_temp_table:
            sql_script_lines.append(f"PRINT 'INFO: Inserting data into Destination [{dest_name}] Table [{target_table}]';")

            # Map input columns of the destination to the columns in the source_temp_table
            # Destination component has <input> -> <inputColumns> -> <inputColumn>
            # and <externalMetadataColumn> which maps to the actual table column.
            dest_input_cols = {} # Map externalMetadataColumnId -> input lineageId
            dest_col_mappings = {} # Map target table column name -> source temp table column name

            dest_input_elem = find_with_ns(data_flow_task, f".//component[@refId='{final_destination_details['refId']}']//inputs/input")
            if dest_input_elem:
                # First get map from external metadata ID to target table column name
                ext_meta_map = {} # externalMetadataColumnId -> target column name
                ext_meta_cols = findall_with_ns(dest_input_elem,".//externalMetadataColumns/externalMetadataColumn")
                for ext_col in ext_meta_cols:
                    ext_meta_map[ext_col.get('id')] = ext_col.get('name')

                # Now map input columns using their lineageId and externalMetadataColumnId
                for in_col in findall_with_ns(dest_input_elem, ".//inputColumns/inputColumn"):
                    lineage_id = in_col.get('lineageId')
                    ext_meta_id = in_col.get('externalMetadataColumnId')
                    target_col_name = ext_meta_map.get(ext_meta_id)

                    # Find the name of the column in the source_temp_table using the lineageId
                    source_col_info = lineage_map.get(lineage_id)
                    source_col_name = source_col_info.get('name') if source_col_info else None


                    if target_col_name and source_col_name:
                        dest_col_mappings[target_col_name] = source_col_name
                    else:
                        print(f"Warning: Could not map input column (LineageID: {lineage_id}, ExternalMetaID: {ext_meta_id}) to target/source column for destination '{dest_name}'.", file=sys.stderr)


            if not dest_col_mappings:
                sql_script_lines.append(f"-- WARNING: Could not determine column mappings for destination '{dest_name}'. Assuming columns match by name/order (*).")
                sql_script_lines.append(f"INSERT INTO {target_table}")
                sql_script_lines.append(f"SELECT * FROM {source_temp_table};")
            else:
                target_cols_str = ", ".join([f"[{c}]" for c in dest_col_mappings.keys()])
                source_cols_str = ", ".join([f"[{c}]" for c in dest_col_mappings.values()])
                sql_script_lines.append(f"INSERT INTO {target_table} ({target_cols_str})")
                sql_script_lines.append(f"SELECT {source_cols_str}")
                sql_script_lines.append(f"FROM {source_temp_table};")

            sql_script_lines.append(f"PRINT 'INFO: Inserted ' + CAST(@@ROWCOUNT AS VARCHAR) + ' rows into [{target_table}].';")
            sql_script_lines.append("")


    # --- Add Cleanup ---
    if generated_temp_tables:
        sql_script_lines.append("-- Clean up temporary tables")
        sql_script_lines.append("PRINT 'INFO: Cleaning up temporary tables...';")
        for table_name in generated_temp_tables:
             # Check existence before dropping
             sql_script_lines.append(f"IF OBJECT_ID('tempdb..{table_name}') IS NOT NULL")
             sql_script_lines.append(f"  DROP TABLE {table_name};")
        sql_script_lines.append("PRINT 'INFO: Cleanup complete.';\n")

    return "\n".join(sql_script_lines)


# --- Main Execution ---
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python convert_ssis_to_sql.py <path_to_dtsx_file> [output_sql_file]")
        sys.exit(1)

    dtsx_file = sys.argv[1]
    output_file = None
    if len(sys.argv) > 2:
        output_file = sys.argv[2]
    else:
        base_name = os.path.splitext(os.path.basename(dtsx_file))[0]
        output_file = f"{base_name}_generated.sql"

    print(f"Input SSIS Package: {dtsx_file}")
    print(f"Output SQL Script: {output_file}")

    generated_sql = analyze_ssis_package(dtsx_file)

    if generated_sql:
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(generated_sql)
            print(f"\nSuccessfully generated SQL script: {output_file}")
            # Print generated SQL to console as well
            print("\n--- Generated SQL ---")
            print(generated_sql)
            print("---------------------\n")
            print("NOTE: Please carefully review the generated SQL script.")
            print("Manual adjustments are likely required, especially for:")
            print("  - Complex SSIS expression translations")
            print("  - Connection managers (server/database names)")
            print("  - Variable usage")
            print("  - Error handling logic")
            print("  - Components not explicitly supported by this script")

        except IOError as e:
            print(f"\nError writing output file {output_file}: {e}", file=sys.stderr)
    else:
        print("\nSQL script generation failed or produced no output.")
